{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alfred FPL — Eval Loop\n",
    "\n",
    "Run the **real Alfred pipeline** end-to-end and inspect responses, charts, and prompt logs inline.\n",
    "\n",
    "**Prerequisites:**\n",
    "- `OPENAI_API_KEY` in `.env`\n",
    "- `FPL_DEV_USER_ID` in `.env` (from `seed_demo.py`)\n",
    "- Supabase seeded (`python scripts/sync.py --from-gw 22`)\n",
    "\n",
    "**Workflow:** Run a question → see Alfred's response → check `prompt_logs/` for what the LLM saw → tweak prompts → re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0: Setup\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, Image, HTML\n",
    "\n",
    "# Enable prompt logging\n",
    "os.environ[\"ALFRED_LOG_PROMPTS\"] = \"1\"\n",
    "# Use mini for all calls (cheaper for iteration)\n",
    "os.environ.setdefault(\"ALFRED_USE_ADVANCED_MODELS\", \"false\")\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Load .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "# Register FPL domain + import alfred\n",
    "import alfred_fpl  # noqa: F401 — triggers domain registration\n",
    "from alfred.graph.workflow import run_alfred\n",
    "from alfred.memory.conversation import initialize_conversation\n",
    "from alfred_fpl.config import settings\n",
    "\n",
    "USER_ID = settings.fpl_dev_user_id\n",
    "assert USER_ID, \"FPL_DEV_USER_ID not set in .env — run seed_demo.py first\"\n",
    "print(f\"User: {USER_ID[:8]}...\")\n",
    "print(f\"Model mode: {'mini-only' if os.environ.get('ALFRED_USE_ADVANCED_MODELS') == 'false' else 'advanced'}\")\n",
    "print(\"Prompt logs: prompt_logs/\")\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Conversation state + helper\n",
    "\n",
    "conversation = initialize_conversation()\n",
    "\n",
    "\n",
    "async def ask_alfred(question: str, reset: bool = False) -> str:\n",
    "    \"\"\"Send a question to Alfred and display the response.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question.\n",
    "        reset: If True, start a fresh conversation.\n",
    "    \n",
    "    Returns:\n",
    "        The raw response string.\n",
    "    \"\"\"\n",
    "    global conversation\n",
    "    if reset:\n",
    "        conversation = initialize_conversation()\n",
    "        print(\"[conversation reset]\")\n",
    "    \n",
    "    print(f\"You: {question}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    start = time.time()\n",
    "    response, conversation = await run_alfred(\n",
    "        user_message=question,\n",
    "        user_id=USER_ID,\n",
    "        conversation=conversation,\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Alfred ({elapsed:.1f}s):\")\n",
    "    display(Markdown(response))\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def show_latest_prompt_log():\n",
    "    \"\"\"Display the most recent prompt log file.\"\"\"\n",
    "    log_dir = Path.cwd().parent / \"prompt_logs\"\n",
    "    if not log_dir.exists():\n",
    "        print(\"No prompt_logs/ directory found.\")\n",
    "        return\n",
    "    \n",
    "    logs = sorted(log_dir.glob(\"*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not logs:\n",
    "        # Try .md files\n",
    "        logs = sorted(log_dir.glob(\"*.md\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not logs:\n",
    "        print(\"No log files found in prompt_logs/\")\n",
    "        return\n",
    "    \n",
    "    latest = logs[0]\n",
    "    print(f\"Latest log: {latest.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    content = latest.read_text(encoding=\"utf-8\")\n",
    "    if latest.suffix == \".json\":\n",
    "        try:\n",
    "            data = json.loads(content)\n",
    "            print(json.dumps(data, indent=2)[:3000])\n",
    "        except json.JSONDecodeError:\n",
    "            print(content[:3000])\n",
    "    else:\n",
    "        print(content[:3000])\n",
    "    \n",
    "    if len(content) > 3000:\n",
    "        print(f\"\\n... [{len(content) - 3000} more chars]\")\n",
    "\n",
    "\n",
    "print(\"Helpers ready: ask_alfred(question), show_latest_prompt_log()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scenario 1: Squad View\n",
    "Tests the squad subdomain — should read squad data and display formation/players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"show my squad\", reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Scouting\n",
    "Tests the scouting subdomain — should query players, filter, rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"show me the best value midfielders under 8m\", reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up turn (same conversation)\n",
    "response = await ask_alfred(\"compare the top 2 by form over the last 5 gameweeks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: Fixtures\n",
    "Tests fixture analysis — should compute FDR, maybe produce heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"which teams have the easiest fixtures next 5 GWs?\", reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: League\n",
    "Tests league standings + rival comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"show my league standings\", reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"compare my squad with the league leader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: Transfers\n",
    "Tests transfer planning flow — squad view then forward search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"show my squad\", reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"who are the cheapest performing forwards?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 6: Market\n",
    "Tests market/transfer trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ask_alfred(\"show me the most transferred-in players this week\", reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prompt Inspector\n",
    "View what the LLM actually received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_latest_prompt_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all prompt logs from this session\n",
    "log_dir = Path.cwd().parent / \"prompt_logs\"\n",
    "if log_dir.exists():\n",
    "    logs = sorted(log_dir.iterdir(), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    for f in logs[:20]:\n",
    "        size = f.stat().st_size\n",
    "        print(f\"  {f.name}  ({size:,} bytes)\")\n",
    "else:\n",
    "    print(\"No prompt_logs/ directory yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ad-hoc Testing\n",
    "Type your own questions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = await ask_alfred(\"your question here\", reset=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
