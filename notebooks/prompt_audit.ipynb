{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Audit â€” Systematic Verification\n",
    "\n",
    "Verifies that every code example in `examples.py` actually works when run through the executor.\n",
    "Also cross-references column names in personas against actual enriched views.\n",
    "\n",
    "**Run this after fixing prompts.** All code blocks should execute without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded. Ready to audit.\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from alfred_fpl.bi.data_access import QuerySpec, Filter, fetch_df, fetch_enriched\n",
    "from alfred_fpl.bi.analytics import (\n",
    "    add_rolling_mean, compute_form_trend, compute_differentials,\n",
    "    compute_fixture_difficulty, compute_price_velocity, rank_by,\n",
    ")\n",
    "from alfred_fpl.bi.viz import render_line, render_bar, render_heatmap, render_comparison\n",
    "from alfred_fpl.bi.executor import execute\n",
    "from alfred_fpl.domain.prompts.examples import _EXAMPLES\n",
    "from alfred_fpl.domain.prompts.personas import _PERSONAS\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print('Loaded. Ready to audit.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Code Blocks from examples.py\n",
    "\n",
    "Pull every \\`\\`\\`python block from the examples and catalog them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 code blocks across 12 example entries\n",
      "\n",
      "  squad:analyze [0]: # Compute formation: merge squad with players for position\n",
      "s...\n",
      "  scouting:analyze [0]: df = df_players.copy()\n",
      "df[\"pts_per_m\"] = df[\"total_points\"] ...\n",
      "  scouting:analyze [1]: # Use add_rolling_mean helper instead of manual rolling\n",
      "df =...\n",
      "  scouting:analyze [2]: # Use compute_fixture_difficulty helper for a specific team\n",
      "...\n",
      "  market:analyze [0]: # Filter candidates by position\n",
      "candidates = df_players[df_p...\n",
      "  market:analyze [1]: # Use compute_price_velocity helper on snapshot data\n",
      "velocit...\n",
      "  league:analyze [0]: # Use compute_differentials helper for squad comparison\n",
      "diff...\n",
      "  league:analyze [1]: my_trend = df_league_standings[df_league_standings[\"manager_...\n",
      "  live:analyze [0]: # Merge enriched squad with GW stats for current-GW points\n",
      "#...\n",
      "  fixtures:analyze [0]: # Enriched fixtures already have team short names (home_team...\n",
      "  fixtures:analyze [1]: # Build FDR grid (self-contained)\n",
      "rows = []\n",
      "for _, fix in df...\n"
     ]
    }
   ],
   "source": [
    "# Extract all code blocks from examples\n",
    "code_blocks = []\n",
    "for key, text in _EXAMPLES.items():\n",
    "    blocks = re.findall(r'```python\\n(.*?)```', text, re.DOTALL)\n",
    "    for i, block in enumerate(blocks):\n",
    "        code_blocks.append({\n",
    "            'key': key,\n",
    "            'index': i,\n",
    "            'code': block.strip(),\n",
    "        })\n",
    "\n",
    "print(f'Found {len(code_blocks)} code blocks across {len(_EXAMPLES)} example entries\\n')\n",
    "for cb in code_blocks:\n",
    "    print(f'  {cb[\"key\"]} [{cb[\"index\"]}]: {cb[\"code\"][:60]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data\n",
    "\n",
    "Fetch real data from Supabase to use as executor context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loaded:\n",
      "  df_players: (50, 21)\n",
      "  df_fixtures: (100, 13)\n",
      "  df_squads: (15, 13)\n",
      "  df_league_standings: (50, 11)\n",
      "  df_player_gameweeks: (52, 22)\n",
      "  df_player_snapshots: (4, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load data for all executor contexts\n",
    "df_players = fetch_enriched('players', order_by='total_points', ascending=False, limit=50)\n",
    "df_fixtures = fetch_enriched('fixtures', order_by='gameweek', limit=100)\n",
    "df_squads = fetch_enriched('squad', limit=15)\n",
    "df_league_standings = fetch_df(QuerySpec(table='league_standings', limit=50))\n",
    "\n",
    "# Player gameweeks for top 2 players\n",
    "if len(df_players) >= 2:\n",
    "    top_ids = df_players.head(2)['id'].tolist()\n",
    "    df_player_gameweeks = fetch_df(QuerySpec(\n",
    "        table='player_gameweeks',\n",
    "        filters=[Filter('player_id', 'in', top_ids)],\n",
    "        order_by='gameweek',\n",
    "        limit=100,\n",
    "    ))\n",
    "else:\n",
    "    df_player_gameweeks = pd.DataFrame()\n",
    "\n",
    "# Player snapshots\n",
    "if len(df_players) >= 1:\n",
    "    df_player_snapshots = fetch_df(QuerySpec(\n",
    "        table='player_snapshots',\n",
    "        filters=[Filter('player_id', 'eq', df_players.iloc[0]['id'])],\n",
    "        limit=20,\n",
    "    ))\n",
    "else:\n",
    "    df_player_snapshots = pd.DataFrame()\n",
    "\n",
    "# Build the full context dict (what the executor would have)\n",
    "test_context = {\n",
    "    'df_players': df_players,\n",
    "    'df_fixtures': df_fixtures,\n",
    "    'df_squads': df_squads,\n",
    "    'df_league_standings': df_league_standings,\n",
    "    'df_player_gameweeks': df_player_gameweeks,\n",
    "    'df_player_snapshots': df_player_snapshots,\n",
    "}\n",
    "\n",
    "print('Test data loaded:')\n",
    "for name, df in test_context.items():\n",
    "    print(f'  {name}: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Every Code Block\n",
    "\n",
    "Run each extracted code block through the executor. Report pass/fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PASS] squad:analyze[0]\n",
      "[PASS] scouting:analyze[0]\n",
      "[PASS] scouting:analyze[1]\n",
      "[PASS] scouting:analyze[2]\n",
      "[PASS] market:analyze[0]\n",
      "[PASS] market:analyze[1]\n",
      "[PASS] league:analyze[0]\n",
      "[PASS] league:analyze[1] [1 charts]\n",
      "[PASS] live:analyze[0]\n",
      "[PASS] fixtures:analyze[0]\n",
      "[PASS] fixtures:analyze[1] [1 charts]\n",
      "\n",
      "Results: 11/11 passed, 0 failed\n"
     ]
    }
   ],
   "source": [
    "# Run every code block through the executor\n",
    "results = []\n",
    "\n",
    "# Some code blocks reference variables that would come from prior context.\n",
    "# Provide reasonable defaults so the executor doesn't fail on missing vars.\n",
    "extra_context = {}\n",
    "if not df_fixtures.empty and 'home_team_id' in df_fixtures.columns:\n",
    "    extra_context['target_team_id'] = df_fixtures.iloc[0]['home_team_id']\n",
    "if not df_league_standings.empty and 'manager_id' in df_league_standings.columns:\n",
    "    managers = df_league_standings['manager_id'].unique()\n",
    "    if len(managers) >= 2:\n",
    "        extra_context['my_id'] = managers[0]\n",
    "        extra_context['rival_id'] = managers[1]\n",
    "# For league differential: need two separate squad DataFrames\n",
    "if not df_squads.empty:\n",
    "    half = len(df_squads) // 2\n",
    "    extra_context['df_squads_mine'] = df_squads.iloc[:max(half, 1)]\n",
    "    extra_context['df_squads_rival'] = df_squads.iloc[max(half, 1):]\n",
    "\n",
    "full_context = {**test_context, **extra_context}\n",
    "\n",
    "for cb in code_blocks:\n",
    "    result = execute(cb['code'], context=full_context)\n",
    "    status = 'PASS' if result.error is None else 'FAIL'\n",
    "    results.append({\n",
    "        'key': cb['key'],\n",
    "        'index': cb['index'],\n",
    "        'status': status,\n",
    "        'error': result.error,\n",
    "        'charts': len(result.charts),\n",
    "        'dfs': list(result.dataframes.keys()),\n",
    "    })\n",
    "    print(f'[{status}] {cb[\"key\"]}[{cb[\"index\"]}]', end='')\n",
    "    if result.charts:\n",
    "        print(f' [{len(result.charts)} charts]', end='')\n",
    "    if result.error:\n",
    "        print(f'\\n  ERROR: {result.error[:120]}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "passed = sum(1 for r in results if r['status'] == 'PASS')\n",
    "failed = sum(1 for r in results if r['status'] == 'FAIL')\n",
    "print(f'\\nResults: {passed}/{len(results)} passed, {failed} failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Column Reference Audit\n",
    "\n",
    "Check what column names appear in persona text and verify they exist in enriched views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns per DataFrame:\n",
      "  df_players: ['assists', 'bonus', 'clean_sheets', 'first_name', 'form', 'fpl_id', 'goals_scored', 'id', 'minutes', 'news']...\n",
      "  df_fixtures: ['away_difficulty', 'away_score', 'away_team', 'away_team_id', 'finished', 'fpl_id', 'gameweek', 'home_difficulty', 'home_score', 'home_team']...\n",
      "  df_squads: ['gameweek', 'id', 'is_captain', 'is_vice_captain', 'manager_id', 'manager_name', 'multiplier', 'player_form', 'player_id', 'player_name']...\n",
      "  df_league_standings: ['event_points', 'gameweek', 'id', 'last_rank', 'league_id', 'league_name', 'manager_id', 'manager_name', 'rank', 'team_name']...\n",
      "  df_player_gameweeks: ['assists', 'bonus', 'bps', 'clean_sheets', 'creativity', 'expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'gameweek']...\n",
      "  df_player_snapshots: ['form', 'gameweek', 'id', 'player_id', 'points_per_game', 'price', 'selected_by_percent', 'snapshot_time', 'transfers_in_event', 'transfers_out_event']\n",
      "\n",
      "--- Persona Column References ---\n",
      "\n",
      "squad:analyze\n",
      "  DataFrame refs: ['squads', 'players']\n",
      "\n",
      "scouting:analyze\n",
      "  DataFrame refs: ['players', 'player_gameweeks', 'fixtures']\n",
      "\n",
      "market:analyze\n",
      "  DataFrame refs: ['players', 'player_snapshots', 'fixtures']\n",
      "\n",
      "league:analyze\n",
      "  DataFrame refs: ['squads', 'players', 'league_standings', 'fixtures']\n",
      "\n",
      "live:analyze\n",
      "  DataFrame refs: ['squads', 'player_gameweeks']\n",
      "\n",
      "fixtures:analyze\n",
      "  DataFrame refs: ['fixtures']\n"
     ]
    }
   ],
   "source": [
    "# Audit column references in personas\n",
    "# Check that column names mentioned in analyze personas exist in the relevant DataFrames\n",
    "\n",
    "# Known columns per enriched view\n",
    "view_columns = {}\n",
    "for name, df in test_context.items():\n",
    "    if not df.empty:\n",
    "        view_columns[name] = set(df.columns)\n",
    "\n",
    "print('Available columns per DataFrame:')\n",
    "for name, cols in view_columns.items():\n",
    "    print(f'  {name}: {sorted(cols)[:10]}...' if len(cols) > 10 else f'  {name}: {sorted(cols)}')\n",
    "\n",
    "# Extract column-like references from analyze personas\n",
    "print('\\n--- Persona Column References ---')\n",
    "for subdomain, personas in _PERSONAS.items():\n",
    "    analyze = personas.get('analyze', '')\n",
    "    # Find potential column references (words that look like column names)\n",
    "    # These are words after \"df_\" or in common patterns\n",
    "    df_refs = re.findall(r'df_(\\w+)', analyze)\n",
    "    col_patterns = re.findall(r'[\"\\']([\\w_]+)[\"\\']', analyze)\n",
    "    if df_refs or col_patterns:\n",
    "        print(f'\\n{subdomain}:analyze')\n",
    "        if df_refs:\n",
    "            print(f'  DataFrame refs: {df_refs}')\n",
    "        if col_patterns:\n",
    "            print(f'  Column-like refs: {col_patterns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompt Logging Setup (for live Alfred pipeline)\n",
    "\n",
    "When ready to test through the full Alfred pipeline, enable prompt logging to see what the LLM actually receives and generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No prompt_logs/ directory found.\n",
      "Enable with: ALFRED_LOG_PROMPTS=1 in .env\n",
      "Then run Alfred and check prompt_logs/ for logged prompts.\n"
     ]
    }
   ],
   "source": [
    "# Prompt logging is built into alfred-core.\n",
    "# To enable, add to your .env:\n",
    "#   ALFRED_LOG_PROMPTS=1      # Logs to prompt_logs/ directory (markdown files)\n",
    "#   ALFRED_LOG_TO_DB=1        # Logs to Supabase prompt_logs table\n",
    "#   ALFRED_LOG_KEEP_SESSIONS=4  # Auto-cleanup, keeps last 4 sessions\n",
    "#\n",
    "# After running Alfred, check prompt_logs/ for:\n",
    "#   - System prompt sent to LLM at each step\n",
    "#   - User prompt (includes persona, examples, data context)\n",
    "#   - LLM response (parsed JSON)\n",
    "#   - Token usage\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir = Path('../prompt_logs')\n",
    "if log_dir.exists():\n",
    "    logs = sorted(log_dir.glob('*'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    print(f'Found {len(logs)} prompt log files')\n",
    "    if logs:\n",
    "        latest = logs[0]\n",
    "        print(f'Latest: {latest.name}')\n",
    "        print(f'Size: {latest.stat().st_size / 1024:.1f} KB')\n",
    "        # Show first 500 chars\n",
    "        print(f'\\nPreview:\\n{latest.read_text()[:500]}...')\n",
    "else:\n",
    "    print('No prompt_logs/ directory found.')\n",
    "    print('Enable with: ALFRED_LOG_PROMPTS=1 in .env')\n",
    "    print('Then run Alfred and check prompt_logs/ for logged prompts.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
